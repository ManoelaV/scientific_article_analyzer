{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2176f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "print(\"ğŸ“¦ Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd9e1a",
   "metadata": {},
   "source": [
    "## ğŸ“š Dataset de 9 Artigos CientÃ­ficos\n",
    "\n",
    "Definindo os 9 artigos cientÃ­ficos distribuÃ­dos igualmente entre as 3 Ã¡reas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549cf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de 9 artigos cientÃ­ficos (3 por Ã¡rea)\n",
    "SCIENTIFIC_ARTICLES = {\n",
    "    # MACHINE LEARNING (3 artigos)\n",
    "    \"ml_001\": {\n",
    "        \"title\": \"Deep Learning for Medical Image Analysis: A Comprehensive Survey\",\n",
    "        \"authors\": [\"Dr. Sarah Chen\", \"Prof. Michael Rodriguez\", \"Dr. Lisa Wang\"],\n",
    "        \"abstract\": \"This paper presents a comprehensive survey of deep learning applications in medical image analysis. We review state-of-the-art architectures including CNNs, transformers, and hybrid models for tasks such as disease detection, image segmentation, and diagnostic prediction. Our analysis covers over 200 recent publications and identifies key trends, challenges, and future directions in the field.\",\n",
    "        \"full_text\": \"\"\"Medical image analysis has been revolutionized by deep learning techniques in recent years. Convolutional Neural Networks (CNNs) have shown remarkable performance in various medical imaging tasks, from radiology to pathology. This comprehensive survey examines the evolution of deep learning architectures specifically designed for medical applications.\n",
    "\n",
    "The main challenges addressed include: 1) Limited labeled data in medical domains, 2) Domain shift between different imaging modalities, 3) Interpretability requirements for clinical adoption, 4) Real-time processing constraints in clinical workflows.\n",
    "\n",
    "Our methodology involved systematic review of 200+ papers published between 2020-2024, focusing on peer-reviewed journals and top-tier conferences. We categorized approaches by medical imaging modality (X-ray, MRI, CT, ultrasound) and clinical application (diagnosis, prognosis, treatment planning).\n",
    "\n",
    "Key findings show that transformer-based architectures are gaining traction, with Vision Transformers (ViTs) showing superior performance on large-scale datasets. However, CNNs remain dominant for smaller datasets typical in medical domains. Hybrid approaches combining CNNs and transformers show the most promising results.\n",
    "\n",
    "The study concludes that while deep learning has achieved remarkable success in medical image analysis, challenges remain in clinical deployment, particularly regarding model interpretability, regulatory approval, and integration with existing healthcare systems.\"\"\",\n",
    "        \"scientific_area\": \"machine_learning\",\n",
    "        \"keywords\": [\"deep learning\", \"medical imaging\", \"CNN\", \"transformers\", \"healthcare AI\"],\n",
    "        \"publication_date\": \"2024-03-15\"\n",
    "    },\n",
    "    \n",
    "    \"ml_002\": {\n",
    "        \"title\": \"Federated Learning for Privacy-Preserving Machine Learning in Healthcare\",\n",
    "        \"authors\": [\"Dr. Alex Thompson\", \"Prof. Maria Gonzalez\"],\n",
    "        \"abstract\": \"We propose a novel federated learning framework for healthcare applications that preserves patient privacy while enabling collaborative model training across multiple institutions. Our approach uses differential privacy and secure aggregation to protect sensitive medical data while maintaining model performance comparable to centralized training.\",\n",
    "        \"full_text\": \"\"\"Healthcare data is highly sensitive and subject to strict privacy regulations such as HIPAA and GDPR. Traditional centralized machine learning approaches require sharing raw patient data, which poses significant privacy and security risks. Federated learning offers a promising solution by enabling collaborative model training without data sharing.\n",
    "\n",
    "Our proposed framework addresses key challenges in healthcare federated learning: 1) Non-IID data distribution across hospitals, 2) Varying computational capabilities of participating institutions, 3) Network connectivity issues, 4) Byzantine fault tolerance against malicious participants.\n",
    "\n",
    "The methodology employs a hierarchical federated learning architecture with secure multi-party computation. Local models are trained on institutional data, and only model updates (gradients or parameters) are shared. We implement differential privacy mechanisms to add calibrated noise to gradients, ensuring individual patient privacy.\n",
    "\n",
    "Experimental results on three healthcare datasets (radiology, genomics, electronic health records) demonstrate that our approach achieves 95-98% of centralized model performance while providing strong privacy guarantees. The framework scales to 50+ participating institutions with minimal communication overhead.\n",
    "\n",
    "This work enables unprecedented collaboration in healthcare AI while respecting patient privacy and regulatory requirements, opening new possibilities for large-scale medical research.\"\"\",\n",
    "        \"scientific_area\": \"machine_learning\", \n",
    "        \"keywords\": [\"federated learning\", \"privacy\", \"healthcare\", \"differential privacy\", \"collaborative AI\"],\n",
    "        \"publication_date\": \"2024-01-22\"\n",
    "    },\n",
    "    \n",
    "    \"ml_003\": {\n",
    "        \"title\": \"Explainable AI for Credit Risk Assessment: Balancing Performance and Interpretability\",\n",
    "        \"authors\": [\"Dr. Robert Kim\", \"Prof. Emma Davis\", \"Dr. James Wilson\"],\n",
    "        \"abstract\": \"This study investigates explainable AI techniques for credit risk assessment in financial services. We compare various interpretability methods including SHAP, LIME, and attention mechanisms, evaluating their effectiveness in providing actionable insights for loan approval decisions while maintaining predictive accuracy.\",\n",
    "        \"full_text\": \"\"\"Credit risk assessment is a critical application of machine learning in financial services, where model decisions directly impact both financial institutions and loan applicants. However, regulatory requirements and fairness considerations demand model interpretability, creating tension between predictive performance and explainability.\n",
    "\n",
    "We address the challenge of building accurate yet interpretable credit risk models. Our approach combines ensemble methods with post-hoc explainability techniques and intrinsically interpretable models. The study evaluates multiple interpretability approaches: SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), integrated gradients, and attention-based architectures.\n",
    "\n",
    "The methodology uses a dataset of 500,000 loan applications with 200+ features including traditional credit metrics, alternative data sources, and behavioral indicators. We implement fairness constraints to ensure equitable treatment across demographic groups while maintaining predictive accuracy.\n",
    "\n",
    "Results show that attention-based neural networks provide the best balance of performance (AUC: 0.87) and interpretability. SHAP explanations enable identification of key risk factors and detection of potential bias. The integrated approach reduces manual review time by 40% while improving approval accuracy by 12%.\n",
    "\n",
    "This research demonstrates that explainable AI can enhance both the performance and trustworthiness of financial ML systems, enabling more informed and fair lending decisions.\"\"\",\n",
    "        \"scientific_area\": \"machine_learning\",\n",
    "        \"keywords\": [\"explainable AI\", \"credit risk\", \"SHAP\", \"interpretability\", \"financial ML\"],\n",
    "        \"publication_date\": \"2024-02-08\"\n",
    "    },\n",
    "    \n",
    "    # CLIMATE SCIENCE (3 artigos)\n",
    "    \"cs_001\": {\n",
    "        \"title\": \"Arctic Sea Ice Decline: Multi-Model Projections and Tipping Point Analysis\",\n",
    "        \"authors\": [\"Dr. Anna Petrov\", \"Prof. Erik Hansen\", \"Dr. Sophie Laurent\"],\n",
    "        \"abstract\": \"We analyze Arctic sea ice decline using ensemble projections from 25 climate models under various emission scenarios. Our study identifies potential tipping points in Arctic sea ice coverage and assesses the likelihood of ice-free summers by 2050. The analysis reveals accelerating ice loss with significant implications for global climate patterns.\",\n",
    "        \"full_text\": \"\"\"Arctic sea ice decline represents one of the most visible and consequential impacts of anthropogenic climate change. The Arctic has warmed at twice the global average rate, leading to dramatic reductions in sea ice extent, thickness, and age. Understanding future trajectories of Arctic sea ice is crucial for climate projection and policy planning.\n",
    "\n",
    "Our research addresses uncertainties in sea ice projections by analyzing ensemble outputs from 25 state-of-the-art climate models participating in CMIP6. We focus on identifying potential tipping points - critical thresholds beyond which sea ice loss becomes irreversible or accelerates dramatically.\n",
    "\n",
    "The methodology combines statistical analysis of historical observations (1979-2023) with model projections under SSP scenarios (SSP1-2.6 to SSP5-8.5). We employ change point detection algorithms to identify regime shifts and calculate probabilistic projections of ice-free conditions.\n",
    "\n",
    "Key findings indicate a 50% probability of ice-free September conditions by 2035 under high emission scenarios (SSP5-8.5), advancing previous estimates by 10-15 years. The analysis reveals a potential tipping point at 4Â°C global warming, beyond which Arctic sea ice recovery becomes highly unlikely on human timescales.\n",
    "\n",
    "These results have profound implications for Arctic ecosystems, global weather patterns, and sea level rise, emphasizing the urgent need for aggressive emission reductions to avoid irreversible Arctic changes.\"\"\",\n",
    "        \"scientific_area\": \"climate_science\",\n",
    "        \"keywords\": [\"Arctic sea ice\", \"climate models\", \"tipping points\", \"global warming\", \"CMIP6\"],\n",
    "        \"publication_date\": \"2024-04-10\"\n",
    "    },\n",
    "    \n",
    "    \"cs_002\": {\n",
    "        \"title\": \"Carbon Cycle Feedbacks in Permafrost Regions: Observations and Earth System Model Evaluation\",\n",
    "        \"authors\": [\"Dr. Marcus Johnson\", \"Prof. Ingrid Olsson\", \"Dr. Yuki Tanaka\"],\n",
    "        \"abstract\": \"This study quantifies carbon cycle feedbacks in permafrost regions using a combination of field observations, remote sensing data, and Earth system model simulations. We find that permafrost carbon release is accelerating faster than previously estimated, with significant positive feedbacks to global warming that are underrepresented in current climate models.\",\n",
    "        \"full_text\": \"\"\"Permafrost regions contain approximately 50% of global soil carbon, representing a massive carbon reservoir vulnerable to climate warming. As temperatures rise, permafrost thaw releases stored carbon as CO2 and methane, creating a positive feedback loop that amplifies global warming. Accurate quantification of this feedback is essential for climate projections and carbon budget assessments.\n",
    "\n",
    "Our integrated approach combines multi-year field observations from 15 permafrost sites across Alaska, Canada, and Siberia with satellite-based monitoring and Earth system model evaluation. We focus on quantifying the magnitude and timing of permafrost carbon release under different warming scenarios.\n",
    "\n",
    "The methodology integrates: 1) In-situ measurements of soil temperature, active layer depth, and carbon fluxes, 2) Remote sensing analysis of land surface changes and vegetation dynamics, 3) Biogeochemical modeling of carbon decomposition processes, 4) Comparison with 12 Earth system models from CMIP6.\n",
    "\n",
    "Results show that current models underestimate permafrost carbon release by 20-40%. Observed carbon emissions are 2-3 times higher than model predictions in rapidly warming regions. The study identifies key mechanisms missing from models: thermokarst formation, hydrological changes, and microbial community shifts.\n",
    "\n",
    "These findings suggest that permafrost carbon feedback could contribute an additional 0.2-0.5Â°C of warming by 2100, highlighting the need for improved model representation of permafrost processes in climate projections.\"\"\",\n",
    "        \"scientific_area\": \"climate_science\",\n",
    "        \"keywords\": [\"permafrost\", \"carbon cycle\", \"feedback loops\", \"Earth system models\", \"Arctic\"],\n",
    "        \"publication_date\": \"2024-03-28\"\n",
    "    },\n",
    "    \n",
    "    \"cs_003\": {\n",
    "        \"title\": \"Extreme Weather Attribution: Linking Climate Change to Regional Weather Events\",\n",
    "        \"authors\": [\"Dr. Christina Mueller\", \"Prof. David Brown\", \"Dr. Raj Patel\"],\n",
    "        \"abstract\": \"We develop an improved framework for extreme weather attribution that quantifies the role of anthropogenic climate change in regional weather events. Using advanced statistical methods and high-resolution climate simulations, we demonstrate how human activities have altered the probability and intensity of heat waves, droughts, and extreme precipitation events.\",\n",
    "        \"full_text\": \"\"\"Extreme weather events cause significant societal and economic impacts, raising questions about the role of anthropogenic climate change in their occurrence and intensity. Traditional climate science focused on long-term trends, but recent advances enable attribution of individual extreme events to human influence on the climate system.\n",
    "\n",
    "Our study develops an enhanced attribution framework combining multiple methodological approaches: statistical analysis of observational data, storyline approaches using high-resolution simulations, and probabilistic event attribution using large ensemble simulations. We focus on three event types: heat extremes, hydrological droughts, and extreme precipitation.\n",
    "\n",
    "The methodology employs: 1) Bias-corrected observational datasets spanning 1950-2024, 2) Large ensemble climate simulations with and without human influence, 3) Machine learning techniques for pattern recognition, 4) Probabilistic risk assessment frameworks adapted from insurance industry practices.\n",
    "\n",
    "Analysis of 50 recent extreme events reveals that anthropogenic climate change has made heat extremes 2-10 times more likely and increased their intensity by 1-4Â°C. For precipitation extremes, human influence has enhanced intensity by 5-15% in most regions. Drought attribution shows more regional variation, with clear human fingerprints in Mediterranean and southwestern North America.\n",
    "\n",
    "These results provide scientific basis for climate risk assessment, adaptation planning, and potentially climate litigation, helping society understand and respond to the changing nature of extreme weather in a warming world.\"\"\",\n",
    "        \"scientific_area\": \"climate_science\",\n",
    "        \"keywords\": [\"extreme weather\", \"attribution\", \"climate change\", \"statistical analysis\", \"risk assessment\"],\n",
    "        \"publication_date\": \"2024-05-14\"\n",
    "    },\n",
    "    \n",
    "    # BIOTECHNOLOGY (3 artigos)\n",
    "    \"bt_001\": {\n",
    "        \"title\": \"CRISPR-Cas9 Gene Editing for Inherited Retinal Diseases: Clinical Trial Results\",\n",
    "        \"authors\": [\"Dr. Jennifer Lee\", \"Prof. Ahmed Hassan\", \"Dr. Maria Rossi\"],\n",
    "        \"abstract\": \"We report results from the first human clinical trial using CRISPR-Cas9 gene editing to treat Leber congenital amaurosis 10 (LCA10). In vivo editing successfully corrected the CEP290 mutation in retinal cells, leading to measurable improvements in visual function in 8 out of 12 patients with excellent safety profile.\",\n",
    "        \"full_text\": \"\"\"Inherited retinal diseases affect over 2 million people worldwide and represent a leading cause of blindness in children and young adults. Leber congenital amaurosis 10 (LCA10), caused by mutations in the CEP290 gene, results in severe vision impairment from birth. Traditional gene therapy approaches face challenges due to the large size of CEP290, making CRISPR-based approaches attractive for treatment.\n",
    "\n",
    "Our clinical trial (EDIT-101) represents the first in vivo CRISPR application for inherited retinal disease. The study enrolled 12 patients aged 3-17 years with confirmed CEP290 mutations and severe visual impairment. We developed a novel delivery system using adeno-associated virus (AAV) vectors optimized for retinal cell transduction.\n",
    "\n",
    "The methodology involved: 1) Subretinal injection of CRISPR-Cas9 components targeting the intronic mutation, 2) Comprehensive safety monitoring including ophthalmological exams and systemic assessments, 3) Functional vision testing using electroretinography and visual field analysis, 4) Long-term follow-up over 24 months.\n",
    "\n",
    "Results demonstrate successful gene editing in retinal tissue with 67% of patients (8/12) showing measurable visual improvement. Light sensitivity increased by 2-4 log units in responsive patients. No serious adverse events were attributed to the treatment. Molecular analysis confirmed precise editing at the target site with minimal off-target effects.\n",
    "\n",
    "This breakthrough demonstrates the therapeutic potential of in vivo CRISPR editing for genetic diseases and opens new avenues for treating previously incurable inherited conditions.\"\"\",\n",
    "        \"scientific_area\": \"biotechnology\", \n",
    "        \"keywords\": [\"CRISPR-Cas9\", \"gene editing\", \"retinal disease\", \"clinical trial\", \"gene therapy\"],\n",
    "        \"publication_date\": \"2024-06-03\"\n",
    "    },\n",
    "    \n",
    "    \"bt_002\": {\n",
    "        \"title\": \"Synthetic Biology Approaches for Sustainable Biofuel Production from Algae\",\n",
    "        \"authors\": [\"Dr. Kevin Zhang\", \"Prof. Sarah Mitchell\", \"Dr. Carlos Mendez\"],\n",
    "        \"abstract\": \"We engineer synthetic biological circuits in microalgae to enhance lipid production for biofuel applications. Our approach combines metabolic engineering with synthetic transcriptional circuits to achieve 300% increase in lipid yield while maintaining growth rates. The engineered strains demonstrate scalable production potential with reduced environmental impact.\",\n",
    "        \"full_text\": \"\"\"The growing demand for sustainable energy sources has intensified research into biofuels as alternatives to fossil fuels. Microalgae represent promising feedstock due to their high lipid content, rapid growth rates, and minimal competition with food crops. However, commercial viability requires significant improvements in lipid productivity and production efficiency.\n",
    "\n",
    "Our synthetic biology approach targets key bottlenecks in algal lipid biosynthesis through rational metabolic engineering. We designed synthetic transcriptional circuits that dynamically regulate carbon flux toward lipid accumulation while maintaining cellular viability and growth rates.\n",
    "\n",
    "The methodology included: 1) Computational modeling of metabolic networks to identify engineering targets, 2) Design and construction of synthetic regulatory circuits using standardized biological parts, 3) CRISPR-mediated genome editing to implement metabolic modifications, 4) Optimization of cultivation conditions for enhanced lipid production.\n",
    "\n",
    "Engineered Chlamydomonas reinhardtii strains achieved 45% lipid content (dry weight) compared to 15% in wild-type strains. The synthetic circuits enable temporal control of lipid accumulation, allowing normal growth followed by rapid lipid synthesis. Scale-up experiments in 100L bioreactors demonstrated consistent performance with productivities of 2.3 g/L/day.\n",
    "\n",
    "These advances bring algal biofuels closer to commercial viability, offering a sustainable pathway for renewable fuel production with significantly reduced carbon footprint compared to petroleum-based fuels.\"\"\",\n",
    "        \"scientific_area\": \"biotechnology\",\n",
    "        \"keywords\": [\"synthetic biology\", \"biofuels\", \"algae\", \"metabolic engineering\", \"sustainability\"],\n",
    "        \"publication_date\": \"2024-04-17\"\n",
    "    },\n",
    "    \n",
    "    \"bt_003\": {\n",
    "        \"title\": \"Personalized Cancer Immunotherapy Using AI-Designed CAR-T Cells\",\n",
    "        \"authors\": [\"Dr. Lisa Chen\", \"Prof. Michael O'Connor\", \"Dr. Hiroshi Yamamoto\"],\n",
    "        \"abstract\": \"We develop an AI-driven platform for designing personalized CAR-T cell therapies targeting patient-specific tumor antigens. Machine learning algorithms analyze tumor genomics and protein expression to predict optimal CAR designs. Clinical results show enhanced efficacy and reduced off-tumor toxicity compared to conventional CAR-T approaches.\",\n",
    "        \"full_text\": \"\"\"Chimeric Antigen Receptor T-cell (CAR-T) therapy has revolutionized cancer treatment, particularly for hematological malignancies. However, current CAR-T therapies face limitations including tumor antigen heterogeneity, immune escape, and off-tumor toxicity. Personalized approaches targeting patient-specific tumor antigens offer potential solutions to these challenges.\n",
    "\n",
    "Our AI-driven platform integrates multi-omics tumor analysis with machine learning algorithms to design personalized CAR constructs. The system predicts optimal target antigens, CAR architectures, and dosing strategies for individual patients based on their unique tumor profile and immune system characteristics.\n",
    "\n",
    "The methodology encompasses: 1) Comprehensive tumor profiling using whole exome sequencing, RNA-seq, and proteomics, 2) AI algorithms for antigen selection and CAR design optimization, 3) Rapid CAR-T cell manufacturing using automated systems, 4) Clinical testing in phase I/II trials for refractory solid tumors.\n",
    "\n",
    "Results from 24 patients with advanced solid tumors show 75% overall response rate compared to 30% for conventional CAR-T. Personalized CAR-T cells demonstrated enhanced tumor infiltration and persistence with reduced cytokine release syndrome. The AI platform accurately predicted responders with 85% accuracy based on pre-treatment tumor characteristics.\n",
    "\n",
    "This personalized approach represents a paradigm shift toward precision cancer immunotherapy, potentially expanding CAR-T applications to solid tumors while minimizing adverse effects through patient-specific optimization.\"\"\",\n",
    "        \"scientific_area\": \"biotechnology\",\n",
    "        \"keywords\": [\"CAR-T cells\", \"personalized medicine\", \"cancer immunotherapy\", \"artificial intelligence\", \"precision medicine\"],\n",
    "        \"publication_date\": \"2024-05-29\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“š Dataset criado com {len(SCIENTIFIC_ARTICLES)} artigos:\")\n",
    "for area in ['machine_learning', 'climate_science', 'biotechnology']:\n",
    "    count = sum(1 for article in SCIENTIFIC_ARTICLES.values() if article['scientific_area'] == area)\n",
    "    print(f\"   {area}: {count} artigos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72419e",
   "metadata": {},
   "source": [
    "## ğŸ”§ Processamento de Texto e Chunking\n",
    "\n",
    "Dividindo os artigos em chunks menores para indexaÃ§Ã£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbae01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Limpa e normaliza texto.\"\"\"\n",
    "    # Remover quebras de linha excessivas\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    # Remover espaÃ§os excessivos\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Divide texto em chunks com sobreposiÃ§Ã£o.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if len(chunk.split()) >= 50:  # MÃ­nimo de 50 palavras por chunk\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Processar artigos em chunks\n",
    "all_chunks = {}\n",
    "article_metadata = {}\n",
    "\n",
    "chunk_counter = 0\n",
    "\n",
    "for article_id, article_data in SCIENTIFIC_ARTICLES.items():\n",
    "    # Limpar texto completo\n",
    "    full_text = clean_text(article_data['full_text'])\n",
    "    \n",
    "    # Criar chunks\n",
    "    chunks = create_chunks(full_text)\n",
    "    \n",
    "    # Armazenar chunks\n",
    "    chunk_ids = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        chunk_id = f\"chunk_{chunk_counter:04d}\"\n",
    "        chunk_counter += 1\n",
    "        \n",
    "        all_chunks[chunk_id] = {\n",
    "            \"content\": chunk_text,\n",
    "            \"article_id\": article_id,\n",
    "            \"chunk_index\": i,\n",
    "            \"page_number\": (i // 3) + 1,  # Simular pÃ¡ginas (3 chunks por pÃ¡gina)\n",
    "            \"word_count\": len(chunk_text.split())\n",
    "        }\n",
    "        chunk_ids.append(chunk_id)\n",
    "    \n",
    "    # Metadados do artigo\n",
    "    article_metadata[article_id] = {\n",
    "        \"id\": article_id,\n",
    "        \"title\": article_data['title'],\n",
    "        \"authors\": article_data['authors'],\n",
    "        \"abstract\": article_data['abstract'],\n",
    "        \"full_text\": full_text,\n",
    "        \"scientific_area\": article_data['scientific_area'],\n",
    "        \"keywords\": article_data['keywords'],\n",
    "        \"publication_date\": article_data['publication_date'],\n",
    "        \"chunk_ids\": chunk_ids\n",
    "    }\n",
    "\n",
    "print(f\"âœ… Processamento concluÃ­do:\")\n",
    "print(f\"   ğŸ“„ {len(article_metadata)} artigos processados\")\n",
    "print(f\"   ğŸ“ {len(all_chunks)} chunks criados\")\n",
    "print(f\"   ğŸ“Š MÃ©dia de {len(all_chunks) / len(article_metadata):.1f} chunks por artigo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7614862",
   "metadata": {},
   "source": [
    "## ğŸ§  GeraÃ§Ã£o de Embeddings\n",
    "\n",
    "Criando embeddings semÃ¢nticos para busca e similaridade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelo de embeddings\n",
    "print(\"ğŸ”„ Carregando modelo de embeddings...\")\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Modelo carregado\")\n",
    "\n",
    "# Preparar textos para embedding\n",
    "chunk_texts = []\n",
    "chunk_ids_ordered = []\n",
    "\n",
    "for chunk_id, chunk_data in all_chunks.items():\n",
    "    chunk_texts.append(chunk_data['content'])\n",
    "    chunk_ids_ordered.append(chunk_id)\n",
    "\n",
    "print(f\"ğŸ”„ Gerando embeddings para {len(chunk_texts)} chunks...\")\n",
    "\n",
    "# Gerar embeddings (pode demorar alguns minutos)\n",
    "embeddings = encoder.encode(chunk_texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"âœ… Embeddings gerados:\")\n",
    "print(f\"   ğŸ“ DimensÃ£o: {embeddings.shape}\")\n",
    "print(f\"   ğŸ’¾ Tamanho: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a73cac",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Salvando Vector Store\n",
    "\n",
    "Persistindo dados para uso pelo sistema MCP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diretÃ³rio do vector store\n",
    "vector_store_dir = Path(\"vector_store\")\n",
    "vector_store_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Salvar metadados dos artigos\n",
    "articles_file = vector_store_dir / \"articles.json\"\n",
    "with open(articles_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(article_metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Artigos salvos: {articles_file}\")\n",
    "\n",
    "# Salvar dados dos chunks\n",
    "chunks_file = vector_store_dir / \"chunks.json\"\n",
    "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Chunks salvos: {chunks_file}\")\n",
    "\n",
    "# Salvar embeddings\n",
    "embeddings_file = vector_store_dir / \"embeddings.npy\"\n",
    "np.save(embeddings_file, embeddings)\n",
    "\n",
    "print(f\"âœ… Embeddings salvos: {embeddings_file}\")\n",
    "\n",
    "# Salvar mapeamento chunk_id -> Ã­ndice\n",
    "mapping_file = vector_store_dir / \"chunk_mapping.json\"\n",
    "chunk_mapping = {chunk_id: idx for idx, chunk_id in enumerate(chunk_ids_ordered)}\n",
    "with open(mapping_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_mapping, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Mapeamento salvo: {mapping_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74b6cd",
   "metadata": {},
   "source": [
    "## ğŸ“Š EstatÃ­sticas do Vector Store\n",
    "\n",
    "Analisando os dados criados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstatÃ­sticas por Ã¡rea cientÃ­fica\n",
    "area_stats = {}\n",
    "for area in ['machine_learning', 'climate_science', 'biotechnology']:\n",
    "    articles_in_area = [a for a in article_metadata.values() if a['scientific_area'] == area]\n",
    "    \n",
    "    total_chunks = sum(len(a['chunk_ids']) for a in articles_in_area)\n",
    "    total_words = sum(len(a['full_text'].split()) for a in articles_in_area)\n",
    "    \n",
    "    area_stats[area] = {\n",
    "        \"articles\": len(articles_in_area),\n",
    "        \"chunks\": total_chunks, \n",
    "        \"total_words\": total_words,\n",
    "        \"avg_words_per_article\": total_words // len(articles_in_area) if articles_in_area else 0\n",
    "    }\n",
    "\n",
    "print(\"\\nğŸ“Š ESTATÃSTICAS DO VECTOR STORE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for area, stats in area_stats.items():\n",
    "    print(f\"\\nğŸ”¬ {area.upper()}:\")\n",
    "    print(f\"   ğŸ“š Artigos: {stats['articles']}\")\n",
    "    print(f\"   ğŸ“ Chunks: {stats['chunks']}\")\n",
    "    print(f\"   ğŸ“„ Palavras: {stats['total_words']:,}\")\n",
    "    print(f\"   ğŸ“ˆ MÃ©dia palavras/artigo: {stats['avg_words_per_article']:,}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ TOTAIS:\")\n",
    "print(f\"   ğŸ“š Artigos: {len(article_metadata)}\")\n",
    "print(f\"   ğŸ“ Chunks: {len(all_chunks)}\")\n",
    "print(f\"   ğŸ§  Embeddings: {embeddings.shape[0]} x {embeddings.shape[1]}\")\n",
    "print(f\"   ğŸ’¾ Tamanho total: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(\"\\nâœ… VECTOR STORE CRIADO COM SUCESSO!\")\n",
    "print(\"   Pronto para uso com o servidor MCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d96055",
   "metadata": {},
   "source": [
    "## ğŸ§ª Teste de Busca SemÃ¢ntica\n",
    "\n",
    "Testando a funcionalidade de busca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def test_semantic_search(query: str, top_k: int = 3):\n",
    "    \"\"\"Testa busca semÃ¢ntica no vector store.\"\"\"\n",
    "    print(f\"\\nğŸ” Busca: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Gerar embedding da query\n",
    "    query_embedding = encoder.encode([query])\n",
    "    \n",
    "    # Calcular similaridades\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Obter top-k resultados\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        chunk_id = chunk_ids_ordered[idx]\n",
    "        chunk_data = all_chunks[chunk_id]\n",
    "        article = article_metadata[chunk_data['article_id']]\n",
    "        \n",
    "        print(f\"\\n{i}. Score: {similarities[idx]:.3f}\")\n",
    "        print(f\"   ğŸ“„ Artigo: {article['title'][:60]}...\")\n",
    "        print(f\"   ğŸ”¬ Ãrea: {article['scientific_area']}\")\n",
    "        print(f\"   ğŸ“ ConteÃºdo: {chunk_data['content'][:150]}...\")\n",
    "\n",
    "# Executar testes\n",
    "test_queries = [\n",
    "    \"machine learning medical diagnosis\",\n",
    "    \"climate change Arctic ice\", \n",
    "    \"CRISPR gene editing therapy\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_semantic_search(query)\n",
    "\n",
    "print(\"\\nğŸ¯ Testes de busca semÃ¢ntica concluÃ­dos!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
