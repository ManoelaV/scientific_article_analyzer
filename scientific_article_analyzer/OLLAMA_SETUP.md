# ü¶ô Guia de Configura√ß√£o do Ollama

Este guia mostra como usar o sistema com **Ollama** (IA local e gratuita).

## üì• Passo 1: Instalar Ollama

### Windows
1. Baixe o instalador: https://ollama.com/download/windows
2. Execute o instalador
3. Ollama iniciar√° automaticamente na porta 11434

### Verificar instala√ß√£o
```powershell
ollama --version
```

## üì¶ Passo 2: Baixar um Modelo

Escolha um modelo baseado na sua RAM dispon√≠vel:

```powershell
# Recomendado - R√°pido e leve (4GB RAM)
ollama pull llama3.2

# Alternativas
ollama pull llama3.2:1b      # Ultra leve (1GB RAM)
ollama pull mistral          # Bom equil√≠brio (4GB RAM)
ollama pull llama3.1:8b      # Mais poderoso (8GB RAM)
ollama pull llama3.1:70b     # Melhor qualidade (64GB RAM)
```

## ‚öôÔ∏è Passo 3: Configurar o Sistema

O arquivo `.env` j√° est√° configurado para Ollama:

```env
OPENAI_API_KEY=ollama
OPENAI_API_BASE=http://localhost:11434/v1
OPENAI_MODEL=llama3.2
```

### Trocar de modelo
Edite `OPENAI_MODEL` no arquivo `.env`:
```env
OPENAI_MODEL=mistral           # Para usar Mistral
OPENAI_MODEL=llama3.1:8b       # Para usar Llama 3.1 8B
```

## üöÄ Passo 4: Usar o Sistema

```powershell
python main.py
```

Ou use o exemplo:
```powershell
python exemplo_uso.py
```

## üß™ Testar Conex√£o com Ollama

Primeiro verifique se Ollama est√° rodando:

```powershell
# Verificar se servidor est√° ativo
curl http://localhost:11434/api/version

# Testar com o modelo
ollama run llama3.2 "Hello, who are you?"
```

## üìù Exemplo de Uso

```python
from main import ScientificArticleAnalyzer

# O sistema carrega automaticamente do .env
analyzer = ScientificArticleAnalyzer()
await analyzer.initialize()

# Analisar artigo
article_text = """
Title: Neural Networks for Image Classification

Abstract: This paper presents a novel deep learning architecture...
"""

result = await analyzer.analyze_article(article_text, "text")
print(f"Categoria: {result.classification.category}")
print(f"Problema: {result.extracted_info.problem}")
```

## üîÑ Outras Op√ß√µes de IA

### Groq (API gratuita e r√°pida)
```env
OPENAI_API_KEY=sua-chave-groq-aqui
OPENAI_API_BASE=https://api.groq.com/openai/v1
OPENAI_MODEL=llama-3.1-70b-versatile
```
Obtenha chave gr√°tis: https://console.groq.com/

### LM Studio (Interface Gr√°fica)
```env
OPENAI_API_KEY=lm-studio
OPENAI_API_BASE=http://localhost:1234/v1
OPENAI_MODEL=llama-3.2
```
Download: https://lmstudio.ai/

### OpenAI (API paga)
```env
OPENAI_API_KEY=sk-proj-...
# Remover OPENAI_API_BASE
OPENAI_MODEL=gpt-4o-mini
```

## ‚ö° Dicas de Performance

1. **Para an√°lises r√°pidas**: Use `llama3.2:1b` (mais r√°pido, menos preciso)
2. **Equil√≠brio**: Use `llama3.2` ou `mistral` (padr√£o recomendado)
3. **M√°xima qualidade**: Use `llama3.1:70b` (requer muito RAM)

## üêõ Problemas Comuns

### Erro: "Connection refused"
- Verifique se Ollama est√° rodando: `ollama serve`
- Windows: Ollama inicia automaticamente, mas pode verificar na bandeja do sistema

### Erro: "Model not found"
- Baixe o modelo primeiro: `ollama pull llama3.2`
- Verifique modelos instalados: `ollama list`

### Sistema muito lento
- Use modelo menor: `llama3.2:1b`
- Ou tente Groq (API na nuvem, gratuita e r√°pida)

## üìä Compara√ß√£o de Modelos

| Modelo | RAM M√≠nima | Velocidade | Qualidade | Recomendado para |
|--------|------------|------------|-----------|------------------|
| llama3.2:1b | 1GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Testes r√°pidos |
| llama3.2 | 4GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Uso geral |
| mistral | 4GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | An√°lises t√©cnicas |
| llama3.1:8b | 8GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | An√°lises detalhadas |
| llama3.1:70b | 64GB | üêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√°xima precis√£o |

## ‚úÖ Verificar Configura√ß√£o

```python
import os
from dotenv import load_dotenv

load_dotenv()

print(f"API Key: {os.getenv('OPENAI_API_KEY')}")
print(f"API Base: {os.getenv('OPENAI_API_BASE')}")
print(f"Model: {os.getenv('OPENAI_MODEL')}")
```

Deveria mostrar:
```
API Key: ollama
API Base: http://localhost:11434/v1
Model: llama3.2
```
